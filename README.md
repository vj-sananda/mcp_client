# ðŸš€ MCP Client with Gemini AI

[ðŸ“¢ Subscribe to The AI Language on YouTube!](https://youtube.com/@theailanguage?sub_confirmation=1)

Welcome! This project features multiple MCP clients integrated with **Google Gemini AI** to execute tasks via the **Model Context Protocol (MCP)** â€” with and without LangChain.

Happy building, and donâ€™t forget to subscribe!  


## MCP Client Options

This repository includes **four MCP client options** for various use cases:

| Option | Client Script | LangChain | Config Support | Transport | Tutorial |
|--------|-------------------------------|------------|----------------|-----------|----------|
| 1 | `client.py` | âŒ | âŒ | STDIO | [Legacy Client](https://youtu.be/GAPncIfnDwg) |
| 2 | `langchain_mcp_client.py` | âœ… | âŒ | STDIO | [LangChain Client](https://youtu.be/hccNm88bk6w) |
| 3 | `langchain_mcp_client_wconfig.py` | âœ… | âœ… | STDIO | [Multi-Server](https://youtu.be/nCnBWVv2uTA) |
| 4 | `client_sse.py` | âŒ | âŒ | SSE (Loca & Web) | [SSE Client](https://youtu.be/s0YJNcT1XMA) |

If you want to add or reuse MCP Servers, check out [the MCP Servers repo](https://github.com/modelcontextprotocol/servers).

---

## âœª Features

âœ… Connects to an MCP server (STDIO or SSE)  
âœ… Uses **Google Gemini AI** to interpret user prompts  
âœ… Allows **Gemini to call MCP tools** via server  
âœ… Executes tool commands and returns results  
âœ… (Upcoming) Maintains context and history for conversations  

---

### Running the MCP Client

Choose the appropriate command for your preferred client:

- **Legacy STDIO** â€” `uv run client.py path/to/server.py`
- **LangChain STDIO** â€” `uv run langchain_mcp_client.py path/to/server.py`
- **LangChain Multi-Server STDIO** â€” `uv run langchain_mcp_client_wconfig.py path/to/config.json`
- **SSE Client** â€” `uv run client_sse.py sse_server_url`

### Project Structure

```
mcp-client-gemini/
â”œâ”€â”€ client.py                        # Basic client (STDIO)
â”œâ”€â”€ langchain_mcp_client.py         # LangChain + Gemini
â”œâ”€â”€ langchain_mcp_client_wconfig.py # LangChain + config.json (multi-server)
â”œâ”€â”€ client_sse.py                   # SSE transport client (local or remote)
â”œâ”€â”€ .env                            # API key environment file
â”œâ”€â”€ README.md                       # Project documentation
â”œâ”€â”€ requirements.txt                # Dependency list
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ LICENSE                         # License information
```

### How It Works

1. You send a prompt:
   > Create a file named test.txt
2. The prompt is sent to **Google Gemini AI**.
3. Gemini uses available **MCP tools** to determine a response.
4. The selected tool is executed on the **connected server**.
5. The AI returns results and maintains **conversation context** (if supported).

### Contributing

Want to help others learn MCP + Gemini?

âœ… Add new client implementations  
âœ… Report bugs or request features  
âœ… Improve this documentation  

Just **fork the repo** and submit a **pull request**!

